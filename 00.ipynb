{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b399fe72",
      "metadata": {
        "id": "b399fe72",
        "outputId": "bc296306-fe5a-43e0-9c97-855db775f89f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AMP-potency-prediction-EvoGradient'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 97 (delta 30), reused 88 (delta 26), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (97/97), 17.16 MiB | 22.91 MiB/s, done.\n",
            "Resolving deltas: 100% (30/30), done.\n",
            "/content/AMP-potency-prediction-EvoGradient/AMP-potency-prediction-EvoGradient/AMP-potency-prediction-EvoGradient/AMP-potency-prediction-EvoGradient/AMP-potency-prediction-EvoGradient/AMP-potency-prediction-EvoGradient\n"
          ]
        }
      ],
      "source": [
        "# clone our github repo\n",
        "# %rm -r ./AMP-potency-prediction-EvoGradient/\n",
        "!git clone https://github.com/MicroResearchLab/AMP-potency-prediction-EvoGradient.git\n",
        "%cd AMP-potency-prediction-EvoGradient/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \\\n",
        "  biopython==1.81 \\\n",
        "# !python AMP_classification.py --testPath './data/classification/demo.fasta' --savePath 'output/classification_result.csv'"
      ],
      "metadata": {
        "id": "MCIdXB67Vqf5",
        "outputId": "40e9bb22-af47-4158-9ba4-f32b04f0dc36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        }
      },
      "id": "MCIdXB67Vqf5",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
            "Requirement already satisfied: torch==1.13.0 in /usr/local/lib/python3.11/dist-packages (1.13.0)\n",
            "\u001b[31mERROR: Ignored the following yanked versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3, 0.15.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision==0.14.0 (from versions: 0.1.6, 0.2.0, 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.17.2, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 0.20.0, 0.20.1, 0.21.0, 0.22.0, 0.22.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchvision==0.14.0\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: biopython==1.81 in /usr/local/lib/python3.11/dist-packages (1.81)\n",
            "Collecting numpy==1.23\n",
            "  Using cached numpy-1.23.0.tar.gz (10.7 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: numpy\n",
            "  Building wheel for numpy (pyproject.toml) ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-25-153475049.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install torch==1.13.0 torchvision==0.14.0 torchaudio==0.13.0 --extra-index-url https://download.pytorch.org/whl/cu116'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install    biopython==1.81    numpy==1.23    # charset-normalizer==3.3.2    # idna==3.6    # pandas==1.3.5    # pillow==9.5.0    # pytz==2024.1    # requests==2.31.0    # typing-extensions==4.7.1    # urllib3==2.0.7    # ipykernel==5.5.5    # ipython==7.33.0    # jupyter_client==5.3.4    # jupyter_core==4.5.0    # prompt-toolkit==3.0.36    # traitlets==5.7.1    # pygments==2.11.2    # parso==0.8.3    # jedi==0.18.1    # python-dateutil==2.8.2    # tornado==5.1.1    # setuptools==65.6.3    # wheel==0.38.4    # six==1.16.0    # wcwidth==0.2.5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"python AMP_classification.py --testPath './data/classification/demo.fasta' --savePath 'output/classification_result.csv'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     return {\n\u001b[1;32m   1086\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".py\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     }\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    605\u001b[0m             make_files(\n\u001b[1;32m    606\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mmake_file\u001b[0;34m(name, hash, size_str)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mmake_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackagePath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFileHash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhash\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhash\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msize_str\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import math\n",
        "import argparse\n",
        "from Bio import SeqIO\n",
        "\n",
        "# Parse command-line arguments\n",
        "parser = argparse.ArgumentParser(description=\"AMP Classification\")\n",
        "parser.add_argument(\"--testPath\", type=str, default='./data/classification/demo.fasta', help=\"Path to the test dataset\")\n",
        "parser.add_argument(\"--savePath\", type=str, default='output/classification_result.csv', help=\"Path to save the results\")\n",
        "args = parser.parse_args()\n",
        "\n",
        "testPath = args.testPath\n",
        "savePath = args.savePath\n",
        "\n",
        "# Data paths\n",
        "trainPath = \"./data/classification/train.csv\"\n",
        "validatePath = \"./data/classification/test.csv\"\n",
        "\n",
        "# Configuration parameters\n",
        "batch_size = 256\n",
        "embedding_size = 20\n",
        "num_tokens = 100\n",
        "num_classes = 2\n",
        "num_heads = 4\n",
        "\n",
        "# Model paths\n",
        "model_list = {\n",
        "    \"CNN\": \"./model/classification/CNN.pth\",\n",
        "    \"Transformer\": \"./model/classification/Transformer.pth\",\n",
        "    \"Attention\": \"./model/classification/Attention.pth\",\n",
        "    \"LSTM\": \"./model/classification/LSTM.pth\",\n",
        "}\n",
        "nameList = model_list.keys()\n",
        "\n",
        "# Sequence to numerical mapping\n",
        "mydict = {\"A\": 0, \"C\": 1, \"D\": 2, \"E\": 3, \"F\": 4, \"G\": 5, \"H\": 6, \"I\": 7, \"K\": 8, \"L\": 9, \"M\": 10, \"N\": 11, \"P\": 12, \"Q\": 13, \"R\": 14, \"S\": 15, \"T\": 16, \"V\": 17, \"W\": 18, \"Y\": 19}\n",
        "\n",
        "softmax = nn.functional.softmax\n",
        "\n",
        "\n",
        "def fasta_to_csv(fasta_path, csv_path):\n",
        "    \"\"\"\n",
        "    Convert a FASTA file to a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    fasta_path (str): Path to the input FASTA file.\n",
        "    csv_path (str): Path to the output CSV file.\n",
        "\n",
        "    Returns:\n",
        "    str: Path to the output CSV file.\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "    lengths = []\n",
        "\n",
        "    # Parse the FASTA file and extract sequences and their lengths\n",
        "    for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
        "        sequences.append(str(record.seq))\n",
        "        lengths.append(len(record.seq))\n",
        "\n",
        "    # Create a DataFrame with sequences and their lengths\n",
        "    df = pd.DataFrame({\"Sequence\": sequences, \"Length\": lengths})\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    print(csv_path)\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    return csv_path\n",
        "\n",
        "\n",
        "# Transform the test FASTA file to CSV\n",
        "testPath = fasta_to_csv(testPath, testPath[:-5] + \".csv\")\n",
        "\n",
        "\n",
        "def dataProcessPipeline(seq):\n",
        "    \"\"\"\n",
        "    Process a sequence into a padded one-hot encoded tensor and a mask.\n",
        "\n",
        "    Parameters:\n",
        "    seq (str): The input sequence to process.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the padded one-hot encoded tensor and the mask tensor.\n",
        "    \"\"\"\n",
        "    testest = seq\n",
        "    num_seq = [mydict[character.upper()] for character in seq]\n",
        "\n",
        "    seq = np.array(num_seq, dtype=int)\n",
        "    len = seq.shape[0]\n",
        "    torch_seq = torch.tensor(seq)\n",
        "\n",
        "    if torch.sum(torch_seq[torch_seq < 0]) != 0:\n",
        "        print(torch_seq[torch_seq < 0])\n",
        "        print(\"wrong seq:\", seq)\n",
        "        print(testest)\n",
        "\n",
        "    onehotSeq = torch.nn.functional.one_hot(torch_seq, num_classes=20)\n",
        "    # Pad the sequence to a length of 100\n",
        "    pad = torch.nn.ZeroPad2d(padding=(0, 0, 0, 100 - len))\n",
        "    mask = np.zeros(100, dtype=int)\n",
        "    mask[len:] = 1\n",
        "    mask = torch.tensor(mask)\n",
        "    pad_seq = pad(onehotSeq)\n",
        "\n",
        "    return pad_seq, mask\n",
        "\n",
        "\n",
        "# train dataset\n",
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        df = pd.read_csv(data_path, header=0)\n",
        "        df = df[df[\"Length\"] <= 100]\n",
        "        self.seqs = list(df[\"Sequence\"])\n",
        "        self.labels = list(df[\"label\"])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        seq = self.seqs[index]\n",
        "        num_seq, mask = dataProcessPipeline(seq)\n",
        "        label = self.labels[index]\n",
        "        return num_seq, mask, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.seqs)\n",
        "\n",
        "\n",
        "# test dataset\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        df = pd.read_csv(data_path, header=0).reset_index()\n",
        "        self.seqs = df[\"Sequence\"]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        seq = self.seqs[index]\n",
        "        num_seq, mask = dataProcessPipeline(seq)\n",
        "        return num_seq, mask, seq\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.seqs)\n",
        "\n",
        "\n",
        "class FastaDataset(Dataset):\n",
        "    def __init__(self, data_path, transform=dataProcessPipeline):\n",
        "        \"\"\"\n",
        "        Initialize the dataset from a FASTA file.\n",
        "\n",
        "        Parameters:\n",
        "        data_path (str): Path to the FASTA file.\n",
        "        transform (function): Function to process the sequences.\n",
        "        \"\"\"\n",
        "        self.seqs = [record.seq for record in SeqIO.parse(data_path, \"fasta\")]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        seq = str(self.seqs[index])\n",
        "        num_seq, mask = self.transform(seq)\n",
        "        return num_seq, mask, seq\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.seqs)\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, length, d_model=20):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(length, d_model)\n",
        "        position = torch.arange(0, length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe\n",
        "        return x\n",
        "\n",
        "\n",
        "\"\"\"attention model\"\"\"\n",
        "\n",
        "\n",
        "class AttentionNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, batch_size=128, embedding_size=20, num_tokens=100, num_classes=2, num_heads=4):\n",
        "\n",
        "        super(AttentionNetwork, self).__init__()\n",
        "        self.pe = PositionalEncoding(len=num_tokens, d_model=embedding_size)\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_tokens = num_tokens\n",
        "        self.num_classes = num_classes\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.hidden1 = 20\n",
        "        self.hidden2 = 60\n",
        "        self.hidden3 = 20\n",
        "        self.dropout = 0.5\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.LN = nn.LayerNorm(normalized_shape=self.hidden1)\n",
        "        self.fc1 = nn.Linear(self.embedding_size, self.hidden1)\n",
        "        self.multihead_att = nn.MultiheadAttention(embed_dim=self.hidden1, num_heads=self.num_heads, batch_first=1, dropout=self.dropout)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc2 = nn.Linear(self.hidden1 * self.num_tokens, self.hidden2)\n",
        "        self.fc3 = nn.Linear(self.hidden2, self.hidden3)\n",
        "        self.fc4 = nn.Linear(self.hidden3, self.num_classes)\n",
        "        self.dropout = nn.Dropout(self.dropout)\n",
        "        self.softmax = nn.functional.softmax\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.pe(x)\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        mask = mask.to(torch.bool)\n",
        "        x, _ = self.multihead_att.forward(x, x, x, key_padding_mask=mask)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc4(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "trainData = TrainDataset(data_path=trainPath)\n",
        "validateData = TrainDataset(data_path=validatePath)\n",
        "testData = TestDataset(data_path=testPath)\n",
        "\n",
        "train_loader = DataLoader(dataset=trainData, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(dataset=testData, batch_size=batch_size, shuffle=False)\n",
        "validate_loader = DataLoader(dataset=validateData, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "result_df = pd.read_csv(testPath, header=0)\n",
        "\n",
        "model_out = {}\n",
        "# process with all the models\n",
        "for modelName in nameList:\n",
        "    modelPath = model_list[modelName]\n",
        "    id = modelPath.split(\"/\")[-2]\n",
        "    model_out[modelName] = []\n",
        "\n",
        "    t_model = torch.load(modelPath, weights_only=False)\n",
        "    t_model.cuda()\n",
        "\n",
        "    # evaluate models\n",
        "    def score(test_loader):\n",
        "        t_model.eval()\n",
        "        epi = 0.000001\n",
        "        tp = 0\n",
        "        tn = 0\n",
        "        fp = 0\n",
        "        fn = 0\n",
        "        total = 0\n",
        "        count = 0\n",
        "\n",
        "        for data in test_loader:\n",
        "            inputs, masks, labels = data\n",
        "            inputs = inputs.float()\n",
        "            masks = masks.float()\n",
        "            inputs, masks, labels = Variable(inputs), Variable(masks), Variable(labels)\n",
        "\n",
        "            inputs = inputs.cuda()\n",
        "            masks = masks.cuda()\n",
        "\n",
        "            if modelName != \"Attention\" and modelName != \"Transformer2\":\n",
        "                out = t_model(inputs)\n",
        "            else:\n",
        "                out = t_model(inputs, masks)\n",
        "            out = torch.squeeze(out)\n",
        "\n",
        "            out = torch.argmax(out, -1)\n",
        "            out = out.cpu()\n",
        "            for i, pre in enumerate(out):\n",
        "                total += 1\n",
        "                if pre == labels[i]:\n",
        "                    count += 1\n",
        "                    if pre == 0:\n",
        "                        tn += 1\n",
        "                    else:\n",
        "                        tp += 1\n",
        "                if pre != labels[i]:\n",
        "                    if pre == 0:\n",
        "                        fn += 1\n",
        "                    else:\n",
        "                        fp += 1\n",
        "\n",
        "        print(\"AMP classification result:\")\n",
        "        print(\"Precision:\", np.round(tp / (tp + fp + epi), 3))\n",
        "        print(\"Recall:\", np.round(tp / (tp + fn + epi), 3))\n",
        "        print(\"Specificity:\", np.round(tn / (tn + fp + epi), 3))\n",
        "        print(\"F1:\", np.round(2 * tp / (2 * tp + fp + fn + epi), 3))\n",
        "        print(\"Accuracy：\", np.round(count / total, 3))\n",
        "        print()\n",
        "\n",
        "    print()\n",
        "    print(\"Model:\", modelName)\n",
        "    score(validate_loader)\n",
        "\n",
        "    # use model to predict test data\n",
        "    for i, data in enumerate(test_loader):\n",
        "        inputs, masks, seqs = data\n",
        "        inputs = inputs.float()\n",
        "        masks = masks.float()\n",
        "\n",
        "        t_model.eval()\n",
        "        inputs = inputs.cuda()\n",
        "        masks = masks.cuda()\n",
        "        if modelName != \"Attention\":\n",
        "            out = t_model(inputs)\n",
        "        else:\n",
        "            out = t_model(inputs, masks)\n",
        "\n",
        "        out = out.cpu()\n",
        "        if \"LSTM\" in modelName:\n",
        "            out = out.unsqueeze(0)\n",
        "        out_ori = torch.squeeze(out)\n",
        "\n",
        "        out_ori = torch.squeeze(out)\n",
        "        out_soft = softmax(out_ori, -1)\n",
        "        out_soft_AMP = out_soft[:, 1]\n",
        "\n",
        "        out_soft_numpy = list(out_soft_AMP.detach().numpy())\n",
        "        out_soft_numpy = [round(v, 3) for v in out_soft_numpy]\n",
        "        model_out[modelName] = list(model_out[modelName]) + out_soft_numpy\n",
        "\n",
        "# summarize the results\n",
        "for k, v in model_out.items():\n",
        "    result_df[k] = v\n",
        "\n",
        "result_df = result_df[[\"Sequence\", \"CNN\", \"Transformer\", \"Attention\", \"LSTM\"]]\n",
        "\n",
        "y = (result_df[\"CNN\"] > 0.5) * (result_df[\"Transformer\"] > 0.5) * (result_df[\"LSTM\"] > 0.5) * (result_df[\"Attention\"] > 0.5)\n",
        "result_df[\"Ensemble\"] = y\n",
        "\n",
        "result_df.to_csv(savePath, index=0)\n",
        "print(result_df)\n",
        "print(f\"Test result is saved to ./{savePath} \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "laYRpQrTb8__",
        "outputId": "ece44ca4-610c-4024-8a23-cd2cc6b8c172"
      },
      "id": "laYRpQrTb8__",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--testPath TESTPATH]\n",
            "                                [--savePath SAVEPATH]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-03f96886-4ddf-460c-8309-eecd668a5955.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}