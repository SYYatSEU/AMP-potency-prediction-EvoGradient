{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b399fe72",
      "metadata": {
        "id": "b399fe72",
        "outputId": "476fc6c6-beab-475c-eea6-d9b86d71f5a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AMP-potency-prediction-EvoGradient'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 97 (delta 30), reused 88 (delta 26), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (97/97), 17.16 MiB | 23.43 MiB/s, done.\n",
            "Resolving deltas: 100% (30/30), done.\n",
            "/content/AMP-potency-prediction-EvoGradient\n"
          ]
        }
      ],
      "source": [
        "# clone our github repo\n",
        "!git clone https://github.com/MicroResearchLab/AMP-potency-prediction-EvoGradient.git\n",
        "%cd AMP-potency-prediction-EvoGradient/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install biopython==1.81"
      ],
      "metadata": {
        "id": "MCIdXB67Vqf5",
        "outputId": "ca206c8b-2831-46d6-f8d2-e299c8a0e9c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MCIdXB67Vqf5",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython==1.81\n",
            "  Downloading biopython-1.81-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython==1.81) (2.0.2)\n",
            "Downloading biopython-1.81-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import math\n",
        "import argparse\n",
        "from Bio import SeqIO\n",
        "\n",
        "# Parse command-line arguments\n",
        "parser = argparse.ArgumentParser(description=\"AMP Classification\")\n",
        "parser.add_argument(\"--testPath\", type=str, default='./data/classification/demo.fasta', help=\"Path to the test dataset\")\n",
        "parser.add_argument(\"--savePath\", type=str, default='output/classification_result.csv', help=\"Path to save the results\")\n",
        "# args = parser.parse_args()\n",
        "args, unknown = parser.parse_known_args()\n",
        "\n",
        "testPath = args.testPath\n",
        "savePath = args.savePath\n",
        "\n",
        "# Data paths\n",
        "trainPath = \"./data/classification/train.csv\"\n",
        "validatePath = \"./data/classification/test.csv\"\n",
        "\n",
        "# Configuration parameters\n",
        "batch_size = 256\n",
        "embedding_size = 20\n",
        "num_tokens = 100\n",
        "num_classes = 2\n",
        "num_heads = 4\n",
        "\n",
        "# Model paths\n",
        "model_list = {\n",
        "    \"CNN\": \"./model/classification/CNN.pth\",\n",
        "    \"Transformer\": \"./model/classification/Transformer.pth\",\n",
        "    \"Attention\": \"./model/classification/Attention.pth\",\n",
        "    \"LSTM\": \"./model/classification/LSTM.pth\",\n",
        "}\n",
        "nameList = model_list.keys()\n",
        "\n",
        "# Sequence to numerical mapping\n",
        "mydict = {\"A\": 0, \"C\": 1, \"D\": 2, \"E\": 3, \"F\": 4, \"G\": 5, \"H\": 6, \"I\": 7, \"K\": 8, \"L\": 9, \"M\": 10, \"N\": 11, \"P\": 12, \"Q\": 13, \"R\": 14, \"S\": 15, \"T\": 16, \"V\": 17, \"W\": 18, \"Y\": 19}\n",
        "\n",
        "softmax = nn.functional.softmax\n",
        "\n",
        "\n",
        "def fasta_to_csv(fasta_path, csv_path):\n",
        "    \"\"\"\n",
        "    Convert a FASTA file to a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    fasta_path (str): Path to the input FASTA file.\n",
        "    csv_path (str): Path to the output CSV file.\n",
        "\n",
        "    Returns:\n",
        "    str: Path to the output CSV file.\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "    lengths = []\n",
        "\n",
        "    # Parse the FASTA file and extract sequences and their lengths\n",
        "    for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
        "        sequences.append(str(record.seq))\n",
        "        lengths.append(len(record.seq))\n",
        "\n",
        "    # Create a DataFrame with sequences and their lengths\n",
        "    df = pd.DataFrame({\"Sequence\": sequences, \"Length\": lengths})\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    print(csv_path)\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    return csv_path\n",
        "\n",
        "\n",
        "# Transform the test FASTA file to CSV\n",
        "testPath = fasta_to_csv(testPath, testPath[:-5] + \".csv\")\n",
        "\n",
        "\n",
        "def dataProcessPipeline(seq):\n",
        "    \"\"\"\n",
        "    Process a sequence into a padded one-hot encoded tensor and a mask.\n",
        "\n",
        "    Parameters:\n",
        "    seq (str): The input sequence to process.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the padded one-hot encoded tensor and the mask tensor.\n",
        "    \"\"\"\n",
        "    testest = seq\n",
        "    num_seq = [mydict[character.upper()] for character in seq]\n",
        "\n",
        "    seq = np.array(num_seq, dtype=int)\n",
        "    len = seq.shape[0]\n",
        "    torch_seq = torch.tensor(seq)\n",
        "\n",
        "    if torch.sum(torch_seq[torch_seq < 0]) != 0:\n",
        "        print(torch_seq[torch_seq < 0])\n",
        "        print(\"wrong seq:\", seq)\n",
        "        print(testest)\n",
        "\n",
        "    onehotSeq = torch.nn.functional.one_hot(torch_seq, num_classes=20)\n",
        "    # Pad the sequence to a length of 100\n",
        "    pad = torch.nn.ZeroPad2d(padding=(0, 0, 0, 100 - len))\n",
        "    mask = np.zeros(100, dtype=int)\n",
        "    mask[len:] = 1\n",
        "    mask = torch.tensor(mask)\n",
        "    pad_seq = pad(onehotSeq)\n",
        "\n",
        "    return pad_seq, mask\n",
        "\n",
        "\n",
        "# train dataset\n",
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        df = pd.read_csv(data_path, header=0)\n",
        "        df = df[df[\"Length\"] <= 100]\n",
        "        self.seqs = list(df[\"Sequence\"])\n",
        "        self.labels = list(df[\"label\"])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        seq = self.seqs[index]\n",
        "        num_seq, mask = dataProcessPipeline(seq)\n",
        "        label = self.labels[index]\n",
        "        return num_seq, mask, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.seqs)\n",
        "\n",
        "\n",
        "# test dataset\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        df = pd.read_csv(data_path, header=0).reset_index()\n",
        "        self.seqs = df[\"Sequence\"]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        seq = self.seqs[index]\n",
        "        num_seq, mask = dataProcessPipeline(seq)\n",
        "        return num_seq, mask, seq\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.seqs)\n",
        "\n",
        "\n",
        "class FastaDataset(Dataset):\n",
        "    def __init__(self, data_path, transform=dataProcessPipeline):\n",
        "        \"\"\"\n",
        "        Initialize the dataset from a FASTA file.\n",
        "\n",
        "        Parameters:\n",
        "        data_path (str): Path to the FASTA file.\n",
        "        transform (function): Function to process the sequences.\n",
        "        \"\"\"\n",
        "        self.seqs = [record.seq for record in SeqIO.parse(data_path, \"fasta\")]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        seq = str(self.seqs[index])\n",
        "        num_seq, mask = self.transform(seq)\n",
        "        return num_seq, mask, seq\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.seqs)\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, length, d_model=20):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(length, d_model)\n",
        "        position = torch.arange(0, length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe\n",
        "        return x\n",
        "\n",
        "\n",
        "\"\"\"attention model\"\"\"\n",
        "\n",
        "\n",
        "class AttentionNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, batch_size=128, embedding_size=20, num_tokens=100, num_classes=2, num_heads=4):\n",
        "\n",
        "        super(AttentionNetwork, self).__init__()\n",
        "        self.pe = PositionalEncoding(len=num_tokens, d_model=embedding_size)\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_tokens = num_tokens\n",
        "        self.num_classes = num_classes\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.hidden1 = 20\n",
        "        self.hidden2 = 60\n",
        "        self.hidden3 = 20\n",
        "        self.dropout = 0.5\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.LN = nn.LayerNorm(normalized_shape=self.hidden1)\n",
        "        self.fc1 = nn.Linear(self.embedding_size, self.hidden1)\n",
        "        self.multihead_att = nn.MultiheadAttention(embed_dim=self.hidden1, num_heads=self.num_heads, batch_first=1, dropout=self.dropout)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc2 = nn.Linear(self.hidden1 * self.num_tokens, self.hidden2)\n",
        "        self.fc3 = nn.Linear(self.hidden2, self.hidden3)\n",
        "        self.fc4 = nn.Linear(self.hidden3, self.num_classes)\n",
        "        self.dropout = nn.Dropout(self.dropout)\n",
        "        self.softmax = nn.functional.softmax\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.pe(x)\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        mask = mask.to(torch.bool)\n",
        "        x, _ = self.multihead_att.forward(x, x, x, key_padding_mask=mask)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc4(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "trainData = TrainDataset(data_path=trainPath)\n",
        "validateData = TrainDataset(data_path=validatePath)\n",
        "testData = TestDataset(data_path=testPath)\n",
        "\n",
        "train_loader = DataLoader(dataset=trainData, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(dataset=testData, batch_size=batch_size, shuffle=False)\n",
        "validate_loader = DataLoader(dataset=validateData, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "result_df = pd.read_csv(testPath, header=0)\n",
        "\n",
        "model_out = {}\n",
        "# process with all the models\n",
        "for modelName in nameList:\n",
        "    modelPath = model_list[modelName]\n",
        "    id = modelPath.split(\"/\")[-2]\n",
        "    model_out[modelName] = []\n",
        "\n",
        "    t_model = torch.load(modelPath, weights_only=False, map_location='cpu')\n",
        "    t_model.cpu()\n",
        "    if modelName == \"Transformer\":\n",
        "        t_model.postion_embedding.device = \"cpu\"\n",
        "    # evaluate models\n",
        "    def score(test_loader):\n",
        "        t_model.eval()\n",
        "        epi = 0.000001\n",
        "        tp = 0\n",
        "        tn = 0\n",
        "        fp = 0\n",
        "        fn = 0\n",
        "        total = 0\n",
        "        count = 0\n",
        "\n",
        "        for data in test_loader:\n",
        "            inputs, masks, labels = data\n",
        "            inputs = inputs.float()\n",
        "            masks = masks.float()\n",
        "            inputs, masks, labels = Variable(inputs), Variable(masks), Variable(labels)\n",
        "\n",
        "            inputs = inputs.cpu()\n",
        "            masks = masks.cpu()\n",
        "\n",
        "            if modelName != \"Attention\" and modelName != \"Transformer2\":\n",
        "                out = t_model(inputs)\n",
        "            else:\n",
        "                out = t_model(inputs, masks)\n",
        "            out = torch.squeeze(out)\n",
        "\n",
        "            out = torch.argmax(out, -1)\n",
        "            out = out.cpu()\n",
        "            for i, pre in enumerate(out):\n",
        "                total += 1\n",
        "                if pre == labels[i]:\n",
        "                    count += 1\n",
        "                    if pre == 0:\n",
        "                        tn += 1\n",
        "                    else:\n",
        "                        tp += 1\n",
        "                if pre != labels[i]:\n",
        "                    if pre == 0:\n",
        "                        fn += 1\n",
        "                    else:\n",
        "                        fp += 1\n",
        "\n",
        "        print(\"AMP classification result:\")\n",
        "        print(\"Precision:\", np.round(tp / (tp + fp + epi), 3))\n",
        "        print(\"Recall:\", np.round(tp / (tp + fn + epi), 3))\n",
        "        print(\"Specificity:\", np.round(tn / (tn + fp + epi), 3))\n",
        "        print(\"F1:\", np.round(2 * tp / (2 * tp + fp + fn + epi), 3))\n",
        "        print(\"Accuracy：\", np.round(count / total, 3))\n",
        "        print()\n",
        "\n",
        "    print()\n",
        "    print(\"Model:\", modelName)\n",
        "    score(validate_loader)\n",
        "\n",
        "    # use model to predict test data\n",
        "    for i, data in enumerate(test_loader):\n",
        "        inputs, masks, seqs = data\n",
        "        inputs = inputs.float()\n",
        "        masks = masks.float()\n",
        "\n",
        "        t_model.eval()\n",
        "        inputs = inputs.cpu()\n",
        "        masks = masks.cpu()\n",
        "        if modelName != \"Attention\":\n",
        "            out = t_model(inputs)\n",
        "        else:\n",
        "            out = t_model(inputs, masks)\n",
        "\n",
        "        out = out.cpu()\n",
        "        if \"LSTM\" in modelName:\n",
        "            out = out.unsqueeze(0)\n",
        "        out_ori = torch.squeeze(out)\n",
        "\n",
        "        out_ori = torch.squeeze(out)\n",
        "        out_soft = softmax(out_ori, -1)\n",
        "        out_soft_AMP = out_soft[:, 1]\n",
        "\n",
        "        out_soft_numpy = list(out_soft_AMP.detach().numpy())\n",
        "        out_soft_numpy = [round(v, 3) for v in out_soft_numpy]\n",
        "        model_out[modelName] = list(model_out[modelName]) + out_soft_numpy\n",
        "\n",
        "# summarize the results\n",
        "for k, v in model_out.items():\n",
        "    result_df[k] = v\n",
        "\n",
        "result_df = result_df[[\"Sequence\", \"CNN\", \"Transformer\", \"Attention\", \"LSTM\"]]\n",
        "\n",
        "y = (result_df[\"CNN\"] > 0.5) * (result_df[\"Transformer\"] > 0.5) * (result_df[\"LSTM\"] > 0.5) * (result_df[\"Attention\"] > 0.5)\n",
        "result_df[\"Ensemble\"] = y\n",
        "\n",
        "result_df.to_csv(savePath, index=0)\n",
        "print(result_df)\n",
        "print(f\"Test result is saved to ./{savePath} \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laYRpQrTb8__",
        "outputId": "2ea3c5ee-b26e-486f-e5a7-922ea7168330"
      },
      "id": "laYRpQrTb8__",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./data/classification/demo..csv\n",
            "\n",
            "Model: CNN\n",
            "AMP classification result:\n",
            "Precision: 0.982\n",
            "Recall: 0.843\n",
            "Specificity: 0.985\n",
            "F1: 0.908\n",
            "Accuracy： 0.916\n",
            "\n",
            "\n",
            "Model: Transformer\n",
            "AMP classification result:\n",
            "Precision: 0.975\n",
            "Recall: 0.845\n",
            "Specificity: 0.98\n",
            "F1: 0.906\n",
            "Accuracy： 0.913\n",
            "\n",
            "\n",
            "Model: Attention\n",
            "AMP classification result:\n",
            "Precision: 0.975\n",
            "Recall: 0.85\n",
            "Specificity: 0.979\n",
            "F1: 0.908\n",
            "Accuracy： 0.916\n",
            "\n",
            "\n",
            "Model: LSTM\n",
            "AMP classification result:\n",
            "Precision: 0.979\n",
            "Recall: 0.865\n",
            "Specificity: 0.982\n",
            "F1: 0.919\n",
            "Accuracy： 0.925\n",
            "\n",
            "                                               Sequence    CNN  Transformer  \\\n",
            "0                             FIHHIIGGLFSAGKAIHRLIRRRRR  0.725        0.816   \n",
            "1                                    MSTNPKPQRKTKRNTNRR  0.381        0.637   \n",
            "2     SDSHLGDLHKKAVPCKDLVPVVVDILVEHFGAARREREEDEEEEQLGGN  0.283        0.213   \n",
            "3     LIDHLGAPRWAVDTILGAIAVGNLASWVLALVPGPGWAVKAGLATA...  0.416        0.521   \n",
            "4     MSGRGKTGGKARAKAKTRSSRAGLQFPVGRVHRLLRKGNYAHRVGA...  0.418        0.499   \n",
            "...                                                 ...    ...          ...   \n",
            "4516                                     LLLFLLKKRKKRKY  0.714        0.708   \n",
            "4517                               WVPAFCQAVGWGDPITHWSH  0.688        0.639   \n",
            "4518                                     INLKALAALAKALL  0.724        0.708   \n",
            "4519  MLTLKKSLLLLFFLGTINLSLCEEERNADEEETRDDPEERDVDVEK...  0.714        0.747   \n",
            "4520                                       MKSTVPYTSRSR  0.336        0.242   \n",
            "\n",
            "      Attention   LSTM  Ensemble  \n",
            "0         0.670  0.733      True  \n",
            "1         0.666  0.537     False  \n",
            "2         0.280  0.267     False  \n",
            "3         0.284  0.462     False  \n",
            "4         0.662  0.509     False  \n",
            "...         ...    ...       ...  \n",
            "4516      0.675  0.723      True  \n",
            "4517      0.673  0.575      True  \n",
            "4518      0.669  0.731      True  \n",
            "4519      0.667  0.765      True  \n",
            "4520      0.371  0.403     False  \n",
            "\n",
            "[4521 rows x 6 columns]\n",
            "Test result is saved to ./output/classification_result.csv \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}